{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import requests \n",
    "import os\n",
    "import glob\n",
    "import dotenv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your .env successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# #insert the location of your .env file here:\n",
    "dotenv.load_dotenv('C:\\\\Users\\\\yujing.wu\\\\OneDrive - World Resources Institute\\\\Documents\\\\Github\\\\cred\\\\.env')\n",
    "\n",
    "#API token needed to make changes\n",
    "API_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "if API_TOKEN:\n",
    "    print('Your .env successfully loaded!')\n",
    "else:\n",
    "    print('Please check the path to your .env file and make sure you have a key called RW_API_KEY in your .env file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'# Resource Watch Dataset Pre-processing Github\\n#### Purpose\\nThis Github repository was created to document the pre-processing done to any dataset displayed on [Resource Watch](https://resourcewatch.org/).\\n\\n#### File Structure\\nThe processing done to each dataset should be stored in a single file, named with the WRI ID and public title used on Resource Watch. This folder should **always** include a README.md file that describes the processing that was done. A template for this file can be found in this repository with the name README_template.md. If a script (preferably Python) was used to process the dataset, that code should also be included as a separate file. The general structure can be summarized as follows:\\n\\n```\\nRepository\\n|\\n|- Dataset 1 folder = {wri_id}_{public_title}\\n| |-{wri_id}_{public_title}_processing.py # optional, script used to process the dataset\\n| |-README.md # file describing the processing\\n| +-...\\n|\\n|-Dataset 2 folder\\n| +-...\\n|\\n+-...\\n```\\n\\n#### Contents of README.md\\nIf the pre-processing was done in Excel, and functions used should be clearly described. If it was done in Carto, any SQL statements should be included as code snippets. For datasets that were processed in Google Earth Engine (GEE), a link to the GEE script should be included, AND the code should be included in the README.md file as a code snippet for users who do not have access to Google Earth Engine.\\n\\nIf the pre-processing was done using a script that has been uploaded to Github, the readme should still be included and describe the general steps that were taken - which datasets were used, how were they modified, etc.\\n\\n#### Contents of script, if included\\nIf a script was used to process the dataset, the code should be uploaded to this Github. This code should be thoroughly commented so that readers unfamiliar with the coding language can still follow the process.\\n\\nAll codes should be written using open-source tools and programming languages. Tools and modules that require a subscription should be avoided (e.g., ArcGIS).\\n\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Github(API_TOKEN)\n",
    "repo = g.get_repo(\"resource-watch/data-pre-processing\")\n",
    "repo.get_readme().decoded_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitRef(ref=\"refs/heads/test_data_team_tool\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the branch \n",
    "branch_name = 'test_data_team_tool'\n",
    "# create a branch based on the master branch \n",
    "repo.create_git_ref(\n",
    "        'refs/heads/{branch_name}'.format(branch_name=branch_name),\n",
    "        repo.get_branch('master').commit.sha\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bio_004a_coral_reef_locations/README.md\n",
      "bio_021a_terrestrial_ecoregions/README.md\n",
      "bio_041_rw1_ocean_health_index/README.md\n",
      "cit_022_rw1_road_traffic_death_rates/README.md\n",
      "cit_029_rw1_municipal_waste/README.md\n",
      "cit_031_rw1_air_quality_PM25_concentration/README.md\n",
      "cit_033a_urban_builtup_area/README.md\n",
      "cit_043_bus_rapid_transit/README.md\n",
      "cit_045_infrastructure_investment_outlook/README.md\n",
      "cli_023_standard_precipitation_index/README.md\n",
      "cli_029a_vulnerability_to_climate_change/README.md\n",
      "cli_030_rw1_aridity/README.md\n",
      "cli_049_rw1_dash_pik_historical_emissions/README.md\n",
      "cli_064_social_cost_carbon/README.md\n",
      "com_002_airports/README.md\n",
      "com_007_rw1_fdi_regulatory_restrictiveness_index/README.md\n",
      "com_017_rw2_major_ports/README.md\n",
      "ene_001a_reservoirs_and_dams/README.md\n",
      "ene_009_renewable_generation_annually/README.md\n",
      "ene_010_renewable_capacity_annually/README.md\n",
      "ene_031a_solar_irradiance/README.md\n",
      "ene_033_energy_consumption/README.md\n",
      "ene_034_electricity_consumption/README.md\n",
      "foo_015a_global_hunger_index/README.md\n",
      "for_001_rw2_tree_cover/README.md\n",
      "for_005a_mangrove/README.md\n",
      "for_007_rw2_tree_cover_gain/README.md\n",
      "for_008_rw2_tree_cover_loss/README.md\n",
      "for_018_rw1_bonn_challenge_restoration_commitment/README.md\n",
      "for_029_peatlands/README.md\n",
      "ocn_001_gebco_bathymetry/README.md\n",
      "ocn_003_projected_sea_level_rise/README.md\n",
      "ocn_005_historical_cyclone_intensity/README.md\n",
      "ocn_006_projected_ocean_acidification/README.md\n",
      "ocn_007_coral_bleaching_monitoring/README.md\n",
      "ocn_008_historical_coral_bleaching_stress_frequency/README.md\n",
      "ocn_009_sea_surface_temperature_variability/README.md\n",
      "ocn_010_projected_coral_bleaching/README.md\n",
      "ocn_012_coral_reef_tourism_value/README.md\n",
      "ocn_013_coral_reef_fisheries_relative_catch/README.md\n",
      "soc_004_rw1_human_development_index/README.md\n",
      "soc_005_rw1_political_rights_civil_liberties_index/README.md\n",
      "soc_006_rw1_multidimensional_poverty_index/README.md\n",
      "soc_025a_gender_inequality_index/README.md\n",
      "soc_037_rw1_malaria_extent/README.md\n",
      "soc_075_male_female_population_densities/README.md\n",
      "soc_091_global_peace_index/README.md\n",
      "soc_092_positive_peace_index/README.md\n",
      "soc_093_global_terrorism_index/README.md\n",
      "wat_026_rw1_wastewater_treatment_plants/README.md\n",
      "wat_036_rw1_water_stress_country_ranking/README.md\n",
      "wat_064_cost_of_sustainable_water_management/README.md\n",
      "wat_066_rw0_conflict_forecast/README.md\n"
     ]
    }
   ],
   "source": [
    "# find the file you want to edit \n",
    "file_type = \"README.md\"\n",
    "#\n",
    "contents = repo.get_contents(\"\", branch_name)\n",
    "files = []\n",
    "for content in contents: \n",
    "    if content.type == 'dir':\n",
    "        file_pathes = [file.path for file in repo.get_contents(content.path, branch_name)]\n",
    "        if any(file_type in file_path for file_path in file_pathes): \n",
    "            path = '/'.join([content.path, file_type])\n",
    "            files.append(repo.get_contents(path, branch_name).path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bio_004a_coral_reef_locations/README.md\n",
      "## Coral Reef Locations Dataset Pre-processing\n",
      "This file describes the data pre-processing that was done to [the Global Distribution of Coral Reefs (2018)](http://data.unep-wcmc.org/datasets/1) for [display on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "The source provided this dataset as two shapefiles - one of which contains polygon data, and the other contains point data.\n",
      "\n",
      "Below, we describe the steps used to reformat the shapefile:\n",
      "1. Read in the polygon shapefile as a geopandas data frame.\n",
      "2. Change the data type of column 'PROTECT', 'PROTECT_FE', and 'METADATA_I' to integers.\n",
      "3. Convert the geometries of the data from shapely objects to geojsons.\n",
      "4. Create a new column from the index of the dataframe to use as a unique id column (cartodb_id) in Carto.\n",
      "\n",
      "Next, a mask layer was created so that it could be overlayed on top of other datasets to highlight where coral reefs were located. In order to create this, a 10km buffer was generated around each coral reef polygon. This was created and exported as a shapefile in Google Earth Engine, using the following code:\n",
      "\n",
      "Please see the [Python script](https://github.com/resource-watch/data-pre-processing/blob/master/bio_004a_coral_reef_locations/bio_004a_coral_reef_locations_processing.py) for more details on this processing.\n",
      "\n",
      "You can view the processed Coral Reef Locations dataset [on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "You can also download the original dataset [directly through Resource Watch](http://wri-public-data.s3.amazonaws.com/resourcewatch/bio_004a_coral_reef_locations.zip), or [from the source website](http://data.unep-wcmc.org/datasets/1).\n",
      "\n",
      "###### Note: This dataset processing was done by [Yujing Wu](https://www.wri.org/profile/yujing-wu), and QC'd by [Amelia Snyder](https://www.wri.org/profile/amelia-snyder).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example file \n",
    "file = files[0]\n",
    "# path to the file\n",
    "print(repo.get_contents(file, ref = branch_name).path)\n",
    "# content of the file \n",
    "ex_content = repo.get_contents(file, ref = branch_name).decoded_content.decode('utf-8')\n",
    "print(ex_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the file \n",
    "# string that needs to be replaced\n",
    "ex_text = 'http://wri-public-data.s3.amazonaws.com'\n",
    "# replacement string \n",
    "replacement_text = 'https://wri-public-data.s3.amazonaws.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Coral Reef Locations Dataset Pre-processing\n",
      "This file describes the data pre-processing that was done to [the Global Distribution of Coral Reefs (2018)](http://data.unep-wcmc.org/datasets/1) for [display on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "The source provided this dataset as two shapefiles - one of which contains polygon data, and the other contains point data.\n",
      "\n",
      "Below, we describe the steps used to reformat the shapefile:\n",
      "1. Read in the polygon shapefile as a geopandas data frame.\n",
      "2. Change the data type of column 'PROTECT', 'PROTECT_FE', and 'METADATA_I' to integers.\n",
      "3. Convert the geometries of the data from shapely objects to geojsons.\n",
      "4. Create a new column from the index of the dataframe to use as a unique id column (cartodb_id) in Carto.\n",
      "\n",
      "Next, a mask layer was created so that it could be overlayed on top of other datasets to highlight where coral reefs were located. In order to create this, a 10km buffer was generated around each coral reef polygon. This was created and exported as a shapefile in Google Earth Engine, using the following code:\n",
      "\n",
      "Please see the [Python script](https://github.com/resource-watch/data-pre-processing/blob/master/bio_004a_coral_reef_locations/bio_004a_coral_reef_locations_processing.py) for more details on this processing.\n",
      "\n",
      "You can view the processed Coral Reef Locations dataset [on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "You can also download the original dataset [directly through Resource Watch](https://wri-public-data.s3.amazonaws.com/resourcewatch/bio_004a_coral_reef_locations.zip), or [from the source website](http://data.unep-wcmc.org/datasets/1).\n",
      "\n",
      "###### Note: This dataset processing was done by [Yujing Wu](https://www.wri.org/profile/yujing-wu), and QC'd by [Amelia Snyder](https://www.wri.org/profile/amelia-snyder).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ex_content.replace(ex_text, replacement_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    repo.update_file(con., \"more tests\", \"more tests\", contents.sha, branch=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pull request "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
