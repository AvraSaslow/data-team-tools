{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Resource Watch GitHub Repo Editor Tool</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "# ! pip install PyGithub\n",
    "from github import Github\n",
    "import requests \n",
    "import os\n",
    "import glob\n",
    "import dotenv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED** <br>\n",
    "<font color=blue>**Enter the path to your .env file below. There should be a GITHUB_TOKEN stored in the .env file that allows you to interact with the GitHub API. If you don't have one yet, go to this [this website](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token) to create one**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your .env successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "# insert the location of your .env file here:\n",
    "dotenv.load_dotenv('C:\\\\Users\\\\yujing.wu\\\\OneDrive - World Resources Institute\\\\Documents\\\\Github\\\\cred\\\\.env')\n",
    "\n",
    "# API token needed to make changes\n",
    "API_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "if API_TOKEN:\n",
    "    print('Your .env successfully loaded!')\n",
    "else:\n",
    "    print('Please check the path to your .env file and make sure you have a key called RW_API_KEY in your .env file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED** <br>\n",
    "<font color=blue>**Enter the GitHub repo you want to edit** <br>\n",
    "Run the next cell to show the readme of the repo you have selected.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Resource Watch Dataset Pre-processing Github\n",
      "#### Purpose\n",
      "This Github repository was created to document the pre-processing done to any dataset displayed on [Resource Watch](https://resourcewatch.org/).\n",
      "\n",
      "#### File Structure\n",
      "The processing done to each dataset should be stored in a single file, named with the WRI ID and public title used on Resource Watch. This folder should **always** include a README.md file that describes the processing that was done. A template for this file can be found in this repository with the name README_template.md. If a script (preferably Python) was used to process the dataset, that code should also be included as a separate file. The general structure can be summarized as follows:\n",
      "\n",
      "```\n",
      "Repository\n",
      "|\n",
      "|- Dataset 1 folder = {wri_id}_{public_title}\n",
      "| |-{wri_id}_{public_title}_processing.py # optional, script used to process the dataset\n",
      "| |-README.md # file describing the processing\n",
      "| +-...\n",
      "|\n",
      "|-Dataset 2 folder\n",
      "| +-...\n",
      "|\n",
      "+-...\n",
      "```\n",
      "\n",
      "#### Contents of README.md\n",
      "If the pre-processing was done in Excel, and functions used should be clearly described. If it was done in Carto, any SQL statements should be included as code snippets. For datasets that were processed in Google Earth Engine (GEE), a link to the GEE script should be included, AND the code should be included in the README.md file as a code snippet for users who do not have access to Google Earth Engine.\n",
      "\n",
      "If the pre-processing was done using a script that has been uploaded to Github, the readme should still be included and describe the general steps that were taken - which datasets were used, how were they modified, etc.\n",
      "\n",
      "#### Contents of script, if included\n",
      "If a script was used to process the dataset, the code should be uploaded to this Github. This code should be thoroughly commented so that readers unfamiliar with the coding language can still follow the process.\n",
      "\n",
      "All codes should be written using open-source tools and programming languages. Tools and modules that require a subscription should be avoided (e.g., ArcGIS).\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a GitHub instance\n",
    "g = Github(API_TOKEN)\n",
    "# get the GitHub repo you want to make change to \n",
    "repo = g.get_repo(\"resource-watch/data-pre-processing\")\n",
    "# print the decoded readme of the selected repo \n",
    "print(repo.get_readme().decoded_content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED** <br>\n",
    "<font color=blue>**Enter a name for the new branch you are creating for the edits** <br>\n",
    "Run the next cell to create a new branch based on the master branch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitRef(ref=\"refs/heads/test_data_team_tool\")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the branch \n",
    "branch_name = 'test_data_team_tool'\n",
    "# create a branch based on the master branch \n",
    "repo.create_git_ref(\n",
    "        'refs/heads/{branch_name}'.format(branch_name=branch_name),\n",
    "        repo.get_branch('master').commit.sha\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED** <br>\n",
    "<font color=blue>**Enter the type of the file you try to edit** <br>\n",
    "Run the next cell to find all the files of this file type.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the type of files you want to edit \n",
    "file_type = \"README.md\"\n",
    "# fetch all the content of the selected github repo\n",
    "contents = repo.get_contents(\"\", branch_name)\n",
    "# create an empty list to store all the files you can find\n",
    "files = []\n",
    "# loop through the directories in the github repo to search for files of this file type \n",
    "for content in contents: \n",
    "    # if it's a directory\n",
    "    if content.type == 'dir':\n",
    "        # go into the directory and put all its content into a list\n",
    "        file_paths = [file.path for file in repo.get_contents(content.path, branch_name)]\n",
    "        # loop through the list of content and store any file of our selected file type to the list created\n",
    "        for file_path in file_paths:\n",
    "            if file_type in file_path:\n",
    "                files.append(repo.get_contents(file_path, branch_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Run the next cell to view the content that you may want to change.<br>\n",
    "We will print the first file of the selected file type to use as an example.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to file: \n",
      "bio_004a_coral_reef_locations/README.md \n",
      "\n",
      "content of file: \n",
      "## Coral Reef Locations Dataset Pre-processing\n",
      "This file describes the data pre-processing that was done to [the Global Distribution of Coral Reefs (2018)](http://data.unep-wcmc.org/datasets/1) for [display on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "The source provided this dataset as two shapefiles - one of which contains polygon data, and the other contains point data.\n",
      "\n",
      "Below, we describe the steps used to reformat the shapefile:\n",
      "1. Read in the polygon shapefile as a geopandas data frame.\n",
      "2. Change the data type of column 'PROTECT', 'PROTECT_FE', and 'METADATA_I' to integers.\n",
      "3. Convert the geometries of the data from shapely objects to geojsons.\n",
      "4. Create a new column from the index of the dataframe to use as a unique id column (cartodb_id) in Carto.\n",
      "\n",
      "Next, a mask layer was created so that it could be overlayed on top of other datasets to highlight where coral reefs were located. In order to create this, a 10km buffer was generated around each coral reef polygon. This was created and exported as a shapefile in Google Earth Engine, using the following code:\n",
      "\n",
      "Please see the [Python script](https://github.com/resource-watch/data-pre-processing/blob/master/bio_004a_coral_reef_locations/bio_004a_coral_reef_locations_processing.py) for more details on this processing.\n",
      "\n",
      "You can view the processed Coral Reef Locations dataset [on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "You can also download the original dataset [directly through Resource Watch](http://wri-public-data.s3.amazonaws.com/resourcewatch/bio_004a_coral_reef_locations.zip), or [from the source website](http://data.unep-wcmc.org/datasets/1).\n",
      "\n",
      "###### Note: This dataset processing was done by [Yujing Wu](https://www.wri.org/profile/yujing-wu), and QC'd by [Amelia Snyder](https://www.wri.org/profile/amelia-snyder).\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the first in the list as an example file \n",
    "file = files[0]\n",
    "# print the path to the file\n",
    "print('path to file: \\n{} \\n'.format(file.path))\n",
    "# content of the file \n",
    "ex_content = file.decoded_content.decode('utf-8')\n",
    "print('content of file: \\n{} \\n'.format(ex_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define our replacement.</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED**</font> <br>\n",
    "<font color=blue>**Enter the text/code you would like to edit (from those printed above) and the replacement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string that needs to be replaced\n",
    "ex_text = 'http://wri-public-data.s3.amazonaws.com'\n",
    "# replacement string \n",
    "replacement_text = 'https://wri-public-data.s3.amazonaws.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED**</font> <br>\n",
    "<font color=blue>**Run this cell and make sure that the replacement text/code (printed below) looks ok for the example file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated content:\n",
      "## Coral Reef Locations Dataset Pre-processing\n",
      "This file describes the data pre-processing that was done to [the Global Distribution of Coral Reefs (2018)](http://data.unep-wcmc.org/datasets/1) for [display on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "The source provided this dataset as two shapefiles - one of which contains polygon data, and the other contains point data.\n",
      "\n",
      "Below, we describe the steps used to reformat the shapefile:\n",
      "1. Read in the polygon shapefile as a geopandas data frame.\n",
      "2. Change the data type of column 'PROTECT', 'PROTECT_FE', and 'METADATA_I' to integers.\n",
      "3. Convert the geometries of the data from shapely objects to geojsons.\n",
      "4. Create a new column from the index of the dataframe to use as a unique id column (cartodb_id) in Carto.\n",
      "\n",
      "Next, a mask layer was created so that it could be overlayed on top of other datasets to highlight where coral reefs were located. In order to create this, a 10km buffer was generated around each coral reef polygon. This was created and exported as a shapefile in Google Earth Engine, using the following code:\n",
      "\n",
      "Please see the [Python script](https://github.com/resource-watch/data-pre-processing/blob/master/bio_004a_coral_reef_locations/bio_004a_coral_reef_locations_processing.py) for more details on this processing.\n",
      "\n",
      "You can view the processed Coral Reef Locations dataset [on Resource Watch](https://resourcewatch.org/data/explore/1d23838e-40da-4cf3-b61c-56258d3a5c56).\n",
      "\n",
      "You can also download the original dataset [directly through Resource Watch](https://wri-public-data.s3.amazonaws.com/resourcewatch/bio_004a_coral_reef_locations.zip), or [from the source website](http://data.unep-wcmc.org/datasets/1).\n",
      "\n",
      "###### Note: This dataset processing was done by [Yujing Wu](https://www.wri.org/profile/yujing-wu), and QC'd by [Amelia Snyder](https://www.wri.org/profile/amelia-snyder).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the edited content of the file\n",
    "updated_content = ex_content.replace(ex_text, replacement_text)\n",
    "print(f'\\nUpdated content:\\n{updated_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loop through all layers and edit the selected field.</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED**</font> <br>\n",
    "<font color=blue>**Create an informative message for the commits. The cell below loops through the list of files we found and replaces the selected text/code with your replacement.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a message that summarizes what you are changing\n",
    "message = \"change http links\"\n",
    "# for each file in the list of files you want to edit\n",
    "for file in files:\n",
    "    # first fetch the decoded content of the file\n",
    "    file_content = file.decoded_content.decode('utf-8')\n",
    "    # then update the file on the branch created previously\n",
    "    repo.update_file(file.path, message, file_content.replace(ex_text, replacement_text), file.sha, branch=branch_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**ACTION REQUIRED**</font> <br>\n",
    "<font color=blue>**Include a title and a summary of the changes you've made on the branch for a pull request. The cell below creates a pull request to merge your branch to the master branch.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the title of the pull request\n",
    "title = \"update all http links in readme files\"\n",
    "# the body of the pull request \n",
    "body = 'this is a test run of the github repo editor tool'\n",
    "# create a pull request\n",
    "pr = repo.create_pull(title=title, body=body, head=branch_name, base=\"master\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
